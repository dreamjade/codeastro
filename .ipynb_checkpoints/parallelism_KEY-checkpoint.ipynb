{"cells":[{"cell_type":"markdown","metadata":{"id":"0td6zaAhvWzy"},"source":["# Parallelism\n","\n","\u003cimg src=\"https://github.com/semaphoreP/codeastro/blob/main/Day4/imgs/parallel_memes.png?raw=1\" style=\"height: 600px;\"/\u003e\n","\n","Computers nowadays all have multiple cores that can run code in parallel. Many codes and algorithms in astronomy can be categorized as \"embarrassingly parallel\", meaning that they can easily be split up into multiple tasks that each have little or no dependency on each other. An example of this is running the same image processing steps on multiple files, or computing the likelihood of a bunch of different sets of models.\n","\n","Parallelism adds extra complexity to the code, making it harder to debug and maintain. Thus, it is important to consider the relative gain of putting in parallelism versus the extra effort in developing and maintaining that code. For this reason, we also recommend that you keep parallelization code as simple as possible. For many tasks, even the most simple parallelism is sufficient.\n","\n","We will discuss how do use parallelism in Python mainly with thread/process pools. There is also some notes on BLAS/LAPACK and GNU Parallel at the end.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5qCMqp0wvWz3"},"outputs":[],"source":["import os\n","import numpy as np\n","import matplotlib.pylab as plt\n","%matplotlib inline\n","\n","import scipy.ndimage as ndimage\n","import timeit\n","import multiprocess as mp\n","import astropy.io.fits as fits"]},{"cell_type":"markdown","metadata":{"id":"kOlgANfivWz4"},"source":["## Running Multiple Python Instances\n","\n","This might sound very unsophisticated, but it is actually a good choice for many instances. Python parallelism is not the best, and if tasks are completely independent of each other, running each task as a separate Python process and saving the result as a file is a perfectly reasonable (and simple) option. This is great for batch processes such as bulk processing a bunch of files with some data reduction code.\n","\n","There are many options to do this: bash script, GNU Parallel, or, as we will focus on today, a master python script. GNU Parallel is also quite useful and is discussed at the end of this notebook. We will focus on using a python script to launch a bunch of python processes because all capabilities of shell scripting (e.g., calling bash commands with the sys module) can be done in Python, and often with much better readability. We will use python to launch a bunch of python processes.\n","\n","We create a function with an argument `index` that tells each proces what chunk of the task to run. We then create a bunch of processes, give them their chunks, and call `start()` to run them. Afterwards, our master process uses `join()` to wait for each process to finish. It is important to always call `join()` at the end to ensure all processes have finished running! If a process has finished immediately, `join()` will immediately return; if a process has not finished, our master process will sleep until the process it is waiting on has finished.\n","\n","## Checking Resource Usage\n","\n","Anytime you are testing parallelized code, it is good to monitor your CPU/RAM usage. Monitoring your CPU usage can help you assess if the number of processes being run in parallel is consistent with what you are looking for. Many times, running parallelized code already involves big data sets, and parallelization will use even more memory (you can think of it as trading runtime for memory usage). So have your resource monitor up occasionally when you are developing this code. It is also a great way to debug the code (e.g., identify hanging parallelization that is not finishing).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8pEDwzZvWz6"},"outputs":[],"source":["mat = np.random.random((3000, 1500)) # 3000x1500 matrix\n","\n","def matrix_loop(mat, index):\n","    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n","    index_start = mat.shape[0] // 10 * index\n","    index_end = mat.shape[0] // 10 * (index + 1)\n","    val  = mat[index_start:index_end].dot(mat.T)\n","    print(\"Process {0} complete\".format(index))\n","    # could save value to a shared variable or save to a file to be used later\n","\n","process_list = []\n","\n","for i in range(10):\n","    p = multiprocessing.Process(target=matrix_loop, args=(mat, i))\n","    process_list.append(p)\n","    p.start()\n","\n","for p in process_list:\n","    p.join()"]},{"cell_type":"markdown","metadata":{"id":"NU16SBuavWz6"},"source":["## Pools\n","\n","Manually creating threads requires a bunch of upkeep code, which is unnecessary if you are just running over a giant loop. In the spirit of keeping parallelism simple, use a high-level parallelization API provided by your programming language whenever possible! It will save you time and effort (Trust me). For dividing up tasks with a for loop, use Python process `Pools`. Essentially, you can give\n","any number of tasks to a process `Pool` and the processes in the pool will loop through and do each one per your instructions.\n","\n","When in doubt about how to parallelize code, use process pools! They are flexible and can accomodate most use cases. And since they have a standardized interface for how to use them, it will make your code more understadable by others compared to home-brewing your own parallelism."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"3CcXtpiNvWz7"},"outputs":[{"name":"stderr","output_type":"stream","text":["Process ForkPoolWorker-11:\n","Traceback (most recent call last):\n"]},{"name":"stdout","output_type":"stream","text":["Created job 0\n","Created job 1\n","Created job 2\n","Created job 3\n","Created job 4\n","Created job 5\n","Created job 6\n","Created job 7\n","Created job 8\n","Created job 9\n"]},{"name":"stderr","output_type":"stream","text":["  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n","    task = get()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n","    return _ForkingPickler.loads(res)\n","AttributeError: Can't get attribute 'matrix_pool' on \u003cmodule '__main__'\u003e\n","Process ForkPoolWorker-12:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n","    task = get()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n","    return _ForkingPickler.loads(res)\n","AttributeError: Can't get attribute 'matrix_pool' on \u003cmodule '__main__'\u003e\n","Process ForkPoolWorker-14:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n","    task = get()\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n","    return _ForkingPickler.loads(res)\n","AttributeError: Can't get attribute 'matrix_pool' on \u003cmodule '__main__'\u003e\n","Process ForkPoolWorker-13:\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 367, in get\n","    return _ForkingPickler.loads(res)\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n","    self.run()\n","  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\n","    self._target(*self._args, **self._kwargs)\n","  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n","    task = get()\n","AttributeError: Can't get attribute 'matrix_pool' on \u003cmodule '__main__'\u003e\n"]},{"name":"stdout","output_type":"stream","text":["Job 4 complete\n","Job 5 complete\n","Job 6 complete\n","Job 7 complete\n","Job 8 complete\n","Job 9 complete\n"]}],"source":["pool = multiprocessing.Pool(processes=4) # creae a pool with 4 worker processes\n","\n","def matrix_pool(mat, index):\n","    # divide up so that we only compute one chunk of the mat.dot(mat.T) matrix\n","    index_start = mat.shape[0] // 10 * index\n","    index_end = mat.shape[0] // 10 * (index + 1)\n","    val  = mat[index_start:index_end].dot(mat.T)\n","    print(\"Job {0} complete\".format(index))\n","\n","    return val # let's return the data this time\n","\n","pool_jobs = []\n","for i in range(10):\n","    job = pool.apply_async(matrix_pool, (mat, i))\n","    pool_jobs.append(job)\n","    print(\"Created job {0}\".format(i))\n","\n","for i, job in enumerate(pool_jobs):\n","    result = job.get()\n","    print(\"Result {0}\".format(i), result.shape)"]},{"cell_type":"markdown","metadata":{"id":"jOgk44wAvWz8"},"source":["# Activity: Image Processing\n","\n","Run the following code to generate some fake data and then run the following image processing function on each fake image and save the resulting data. How fast can you process the data when the images are 1000x1000? What is the speedup from 1 core to multiple cores (is it linear?)?\n","\n","### Instructions\n","\n","  * Run the first cell below to generate 25 images of fake data\n","  * Run the second cell below to run the `process_image()` function on a single image without parallelism. Time how long this takes. Let's call this time t0.\n","  * Write parallelization code to parallelize the `process_image()` function. Time how long it takes to process 25 images. Let's call this tp.\n","  * Compute speedup = (25 * t0) / tp: the speedup factor between doing 25 images in series and parallelizing it.\n","  * (Optional) Try using different number of processes to parallelize the activity. How does the run time change with the number of processes used? How does it relate to the number of cores you have on your computer? Is it linear?\n","\n","### End Product\n","  * Report on Piazza your best speedup factor you got. And specify how many processes you used, and how many cores you have.\n","  * (Optional): make a graph of wall clock time to process 25 images vs number of processes used. Post this on Piazza as well.  \n","\n","### Roles\n","  * Driver: in charge of sharing their screen and typing the code for this activity\n","  * Recorder: in charge of writing down all the results and reporting back when we poll everyone\n","  * Time-keeper: in charge of keeping the team on track to finish the activity\n","  * Navigator: in charge of directing the driver what to code (everyone else; can be more than one person)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SvSfonMvvWz9"},"outputs":[],"source":["# note, this cell might take a minute to run.\n","\n","def generate_fake_data(filename, dims):\n","    \"\"\"\n","    Generates a fake dataframe with random numbers\n","\n","    Args:\n","        filename (str): file to save the data to\n","        dims (tuple): (Ny, Nx) pair that species the size of the y and x dimensions\n","    \"\"\"\n","    # some complicated random image generation. Feel free to ignore.\n","    # coordinate system in fourier spae\n","    u,v = np.meshgrid(np.fft.fftfreq(dims[1]), np.fft.fftfreq(dims[0]))\n","    phases = np.random.uniform(0, 2*np.pi, u.shape)\n","    rho = np.sqrt((u*dims[1])**2 + (v*dims[0])**2)\n","    # suppress high frequency by a squared exponential\n","    spectrum = np.exp(-rho**2/(np.max(rho)/50)**2)  * np.exp(1j * phases)\n","    filtered = np.real(np.fft.ifft2(spectrum))\n","\n","    fits.writeto(filename, filtered, overwrite=True)\n","\n","\n","# generate fake data (can choose to save it somethere else if you want)\n","fileformat = os.path.join(\"./\", \"fake_{0}x{1}_{2}.fits\")\n","\n","ny = 1000\n","nx = 1000\n","for i in range(25):\n","    filename = fileformat.format(ny, nx, i)\n","    generate_fake_data(filename, (ny, nx) )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QnyrBkLIvWz-"},"outputs":[],"source":["def process_image(frame, filtersize=50):\n","    \"\"\"\n","    Run a high-pass filter on the data.\n","    Remove the low spatial frequency (i.e., smooth features) in the image\n","\n","    Args:\n","        frame (np.array): a 2-D image to be processed\n","        fitersize (int): the size of the filter. Features smaller than the filtersize will be preserved\n","\n","    Returns:\n","        processed_frame (np.array): a 2-D image after processing\n","    \"\"\"\n","    # run a median filter to smooth the image\n","    frame_smooth = ndimage.median_filter(frame, filtersize)\n","\n","    processed_frame = frame - frame_smooth\n","\n","    return processed_frame\n","\n","import time\n","\n","start = time.time()\n","\n","# an example of running this on one image\n","with fits.open(fileformat.format(ny, nx, 0)) as hdulist:\n","    data = hdulist[0].data\n","\n","\n","    filt_data = process_image(data)\n","\n","    fig = plt.figure(figsize=(6,3))\n","    ax1 = fig.add_subplot(121)\n","    ax1.imshow(data, cmap=\"inferno\")\n","    ax1.set_title(\"Original\")\n","    ax2 = fig.add_subplot(122)\n","    ax2.imshow(filt_data, cmap=\"inferno\")\n","    ax2.set_title(\"Filtered\")\n","\n","single_time = time.time() - start\n","print('1 image/1 process: {:.2f} seconds'.format(single_time))\n","\n","# a function that runs this on several images in a loop\n","def process_image_loop(index_list):\n","    \"\"\"\n","    Do some image processing in serial on a list of images.\n","\n","    Args:\n","        index_list (np.array of int): indices of images to be processed.\n","    \"\"\"\n","\n","    for index in index_list:\n","\n","        with fits.open(fileformat.format(ny, nx, index)) as hdulist:\n","            data = hdulist[0].data\n","\n","            filt_data = process_image(data)\n","\n","            # we are also optionally plotting and saving the data\n","            fig = plt.figure(figsize=(6,3))\n","            ax1 = fig.add_subplot(121)\n","            ax1.imshow(data, cmap=\"inferno\")\n","            ax1.set_title(\"Original\")\n","            ax2 = fig.add_subplot(122)\n","            ax2.imshow(filt_data, cmap=\"inferno\")\n","            ax2.set_title(\"Filtered\")\n","            plt.savefig('{}.png'.format(index))\n","\n","#### Activity:\n","# write and time some code that does this in parallel. How does the performance increase as you increase the number of processes you use?\n","# we recommend you use multiprocessing pool for this task\n","\n","num_processes = 8\n","\n","start = time.time()\n","\n","# initialize a Pool object\n","pool = multiprocessing.Pool(processes=num_processes)\n","\n","# start the jobs running in parallel\n","pool_jobs = []\n","for i in np.arange(25):\n","    img_indices = [i] # only do one image per job\n","    job = pool.apply_async(process_image_loop, args=(img_indices,))\n","    pool_jobs.append(job)\n","\n","for job in pool_jobs:\n","    result = job.get()\n","\n","# save the\n","parallelized_time = time.time() - start\n","\n","\n","print('25 images/{} processes: {:.2f} seconds'.format(num_processes, parallelized_time))\n","\n","\"\"\"\n","The speedup is not linear, especially when dealing with parallelization tasks that are short (smaller images). This is because it takes some constant amount of time to initialize and join processes, so more processes mean more time at the beginning and end of the computation. This drives home the idea that parallelization is only necessary when the task you're parallelizing is very computationally intensive (i.e. the process start-up \u0026 join times are negligible in comparison).\n","\"\"\""]},{"cell_type":"markdown","metadata":{"id":"0N_JBZrIvWz_"},"source":["# Parallelism Extended Cut\n","\n","There is a lot of talk about in parallelism - we are just scratching the surface. The above concepts _should_ cover the majority of parallelism use cases. However, here are some good things to know.\n","\n","\n","## Do you really need parallelism\n","\n","We already emphasized this above, but parallelism adds complexity. Try avoiding parallelization until it is necessary. If some code takes 10 minutes to run, but you only need to run it once per week, is it worth parallelizing? The programming and upkeep costs may not be worth it. Parallelism also makes it hard to debug: we cannot attach the python debugger on other Python processes meaning they often crash with no error message as to why. GNU Parallel (below) is an alternative to writing more complicated code.\n","\n","## GNU Parallel\n","\n","If you have a script that you want to run multiple times (e.g., on multiple files), you don't need to write Python parallelization. We can use the `parallel` package that can be installed on UNIX systems to handle the parallelization. For example, if we can process a single file by running on the command line:\n","\n","    python process.py file_01.fits\n","\n","Then we can run it on `file_01.fits` to `file_99.fits` by running on the command line:\n","\n","    parallel python process.py ::: file_{01..99}.fits\n","\n","\n","## Threads vs Processes\n","Python has two parallelization modules: `multiprocessing` and `multithreading`. What's the difference? Why do we only use `multiprocessing`?\n","\n","Threads and processes are both tasks that your computer CPUs run in parallel. Threads share the same memory, whereas processes do not. Processes have to communicate themselves with shared variables (see below) or via a slower I/O method (writing to files, communicating over websockets). Most languages use threads to parallelize computations because sharing the same memory is very convenient and saves on resources. However, Python has the \"Global Intepreter Lock\" (GIL) that allows only one thread to run at a time (for complciated consistency reasons). Generally, parallelism in astronomy involves computationally intensive tasks so only one of them being able to run at a time would defeat the purpose. That's why you will generally only see multiprocessing in astronomy-related software development.\n","\n","Threads are more frequently seen outside of astronomy, but it is generally useful for \"I/O bound tasks\" as opposed to the \"CPU bound tasks\" that we usually encounter in astronomy. Anytime you have tasks with a lot of sleeping/waiting time such as if you have multiple web API queries (e.g., multiple database queries) or waiting for files to be created, threads are a better choice because they use less memory and they can access all of your variables instead of having to define shared variables.\n","\n","The one notable exception is that `numpy` functions that call C code releases the GIL. This means if your code is dominated by `numpy` matrix operations such as dot product, then you can actually use threads with minimal increase in runtime.\n","\n","## Shared Variables\n","\n","Processes do not by default share memory. For processes to read/write/access the same variables, we have to declare shared memory. Python `multiprocessing` provides nearly all shared memory structure that you should be using: queues and arrays. Queues allow for interprocess communication. Arrays allow for large datasets to be accessed by multiple processes without needing to duplicate them (possibly saving a lot of memory usage). Use these special arrays and queues becasue they are automatically synchronized between processes and do not require synchronization code, which is generally low-level synchronization that we should avoid programming unless we absolutely need it.\n"]},{"cell_type":"markdown","metadata":{"id":"mjnwCAHXvW0A"},"source":["## BLAS/LAPACK/ATLAS/MKL\n","\n","This often isn't the parallelism you are looking for, but it is a way to speed up single-threaded code that relies heavily on matrix operations.\n","\n","There are packages that automatically parallelize linear algebra routines like matrix dot product or singular value decomposition. BLAS/LAPACK are the algorithms, and ATLAS, OpenBLAS, or MKL are implementations of them. If you have an Intel processor, you will be using MKL. They are quite useful for when you don't want to write your own parallelization code but want to speed up matrix routines. It requires numpy to be configured for BLAS/MKL properly. Generally if you install numpy with Anaconda, this happens automatically (you should already be benefitting from it), so we recommend simply doing it this way. These speedups are turned on by default when configured.\n","\n","However, if you wish to do your own parallelism, make sure to turn this off. Having multiple layers of parallelism will often slow down your code!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P3CV3nZwvW0B"},"outputs":[],"source":["# Check we have BLAS/LAPACK. Notice they are from the MKL library\n","np.show_config()"]},{"cell_type":"markdown","metadata":{"id":"F1py2Z3wvW0B"},"source":["In the code block below, we are using the MKL package which works with Intel CPUs. If you do not have an Intel CPU, this command will not do anything. You would need to configure BLAS with environment variables (MKL can be done this way too, but it affects your entire user, rather than just this python script locally). Here's an example to modify it in bash to turn off BLAS parallelization:\n","\n","    export OPENBLAS_NUM_THREADS=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUny7n3xvW0B"},"outputs":[],"source":["##### Note, this following demo only works if you can use MKL.\n","import mkl\n","\n","mat = np.random.random((1000, 1500)) # 1000x1500 matrix\n","\n","# Use 4 cores to parallelize matrix operations\n","mkl.set_num_threads(4)\n","print(\"4 cores\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))\n","\n","# Disable MKL\n","mkl.set_num_threads(1)\n","print(\"1 core\", timeit.timeit(\"mat.dot(mat.T)\", setup=\"from __main__ import mat\", number=100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oupB1Qg7vW0C"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"interpreter":{"hash":"e88b2effbe4898ca25755c3c375167d4697d227cadbe1b7a4bf6828a259c304a"},"kernelspec":{"display_name":"Python 3.9.5 64-bit ('codeastro': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}